{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten,BatchNormalization,Conv2D,MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=r\"C:\\Users\\Ashok\\puthon\\Great Learning\\Transfer Learning\\17flowers\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 500, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test a image shape\n",
    "test_image=cv2.imread(r\"C:\\Users\\Ashok\\puthon\\Great Learning\\Transfer Learning\\17flowers\\train\\1\\image_0082.jpg\",1)\n",
    "cv2.imshow('test',test_image)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Data - Split Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder=r\"C:\\Users\\Ashok\\puthon\\Great Learning\\Transfer Learning\\17flowers\\train\"\n",
    "test_folder=r\"C:\\Users\\Ashok\\puthon\\Great Learning\\Transfer Learning\\17flowers\\test\"\n",
    "validation_folder=r\"C:\\Users\\Ashok\\puthon\\Great Learning\\Transfer Learning\\17flowers\\validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,(dirpath,dirnames,filenames) in enumerate ((os.walk(dataset_path))):\n",
    "    if dirpath is not dataset_path:\n",
    "        print(dirpath)\n",
    "        \n",
    "        #Create folders in test path under each class\n",
    "        class_folder=dirpath.split(\"\\\\\")[-1]\n",
    "        test_path=os.path.join(test_folder,class_folder)\n",
    "        os.mkdir(test_path)\n",
    "        #create folders in validation path under each class\n",
    "        validation_path=os.path.join(validation_folder,class_folder)\n",
    "        os.mkdir(validation_path)\n",
    "        \n",
    "        train_path=os.path.join(train_folder,class_folder)\n",
    "        \n",
    "        n_files=int(len(os.listdir(train_path))*20/100)\n",
    "        \n",
    "        for c in random.sample(os.listdir(train_path),n_files):\n",
    "            shutil.move(os.path.join(dirpath,c),test_path)\n",
    "            \n",
    "        for c in random.sample(os.listdir(train_path),n_files):\n",
    "            shutil.move(os.path.join(dirpath,c),validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=os.listdir(train_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 827 images belonging to 17 classes.\n",
      "Found 274 images belonging to 17 classes.\n",
      "Found 274 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches=ImageDataGenerator(preprocessing_function=None)\\\n",
    ".flow_from_directory(directory=train_folder,target_size=(224,224),classes=classes,batch_size=10)\n",
    "\n",
    "validation_batches=ImageDataGenerator(preprocessing_function=None)\\\n",
    ".flow_from_directory(directory=validation_folder,target_size=(224,224),classes=classes,batch_size=10)\n",
    "\n",
    "test_batches=ImageDataGenerator(preprocessing_function=None)\\\n",
    ".flow_from_directory(directory=test_folder,target_size=(224,224),classes=classes,batch_size=10,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,labels=next(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model=Sequential([\n",
    "    Conv2D(filters=32,kernel_size=(3,3),activation='relu',padding='same',input_shape=(224,224,3)),\n",
    "    MaxPool2D(pool_size=(2,2),strides=2),\n",
    "    Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'),\n",
    "    MaxPool2D(pool_size=(2,2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=17,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 23s 266ms/step - loss: 266.7020 - accuracy: 0.0821 - val_loss: 2.7379 - val_accuracy: 0.1350\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 22s 262ms/step - loss: 1.7143 - accuracy: 0.5882 - val_loss: 3.2319 - val_accuracy: 0.1971\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 22s 264ms/step - loss: 0.4566 - accuracy: 0.8874 - val_loss: 3.8446 - val_accuracy: 0.2190\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 22s 266ms/step - loss: 0.1177 - accuracy: 0.9730 - val_loss: 4.9285 - val_accuracy: 0.2226\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 22s 267ms/step - loss: 0.0640 - accuracy: 0.9882 - val_loss: 4.6622 - val_accuracy: 0.2117\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 22s 267ms/step - loss: 0.0317 - accuracy: 0.9950 - val_loss: 4.8200 - val_accuracy: 0.2226\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 23s 279ms/step - loss: 0.0453 - accuracy: 0.9873 - val_loss: 5.0789 - val_accuracy: 0.2445\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 23s 271ms/step - loss: 0.0855 - accuracy: 0.9956 - val_loss: 6.1770 - val_accuracy: 0.2117\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 24s 285ms/step - loss: 0.0442 - accuracy: 0.9936 - val_loss: 5.5416 - val_accuracy: 0.1861\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 24s 288ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 7.5592 - val_accuracy: 0.2372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1be5a921370>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(x=train_batches,validation_data=validation_batches,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Net\n",
    "from keras.layers import Input,Lambda,Dense,Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "image_input=Input(shape=(224,224,3))\n",
    "model=VGG16(input_tensor=image_input , weights='imagenet',include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#make the layers in the VGG net non trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#vgg.include_top=True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,Conv2D,MaxPool2D,Activation,Dropout,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "block5_pool =model.get_layer('block5_pool').output\n",
    "block6_conv1=Conv2D(filters=32,kernel_size=(3,3),strides=(2,2),padding='valid',data_format='channels_last',\n",
    "            activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=0),name='block6_conv1')(block5_pool)\n",
    "block6_pool=MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid',data_format='channels_last',\n",
    "               name='block6_pool')(block6_conv1)\n",
    "\n",
    "#Flatten\n",
    "fatten=Flatten(data_format='channels_last',name='Flatten')(block6_pool)\n",
    "\n",
    "#FC layer\n",
    "FC1=Dense(units=32,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal\n",
    "         (seed=32),name='FC1')(fatten)\n",
    "\n",
    "out=Dense(units=17,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal\n",
    "         (seed=32),name='out')(FC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "block6_conv1 (Conv2D)        (None, 3, 3, 32)          147488    \n",
      "_________________________________________________________________\n",
      "block6_pool (MaxPooling2D)   (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 17)                561       \n",
      "=================================================================\n",
      "Total params: 14,863,793\n",
      "Trainable params: 149,105\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_model=Model(image_input,out)\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "custom_model.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "83/83 [==============================] - 139s 2s/step - loss: 8.6077 - accuracy: 0.1833 - val_loss: 2.2641 - val_accuracy: 0.4015\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 1.4898 - accuracy: 0.5818 - val_loss: 1.9698 - val_accuracy: 0.5146\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 142s 2s/step - loss: 0.6302 - accuracy: 0.8221 - val_loss: 1.9794 - val_accuracy: 0.6058\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 144s 2s/step - loss: 0.3721 - accuracy: 0.8919 - val_loss: 1.7200 - val_accuracy: 0.6350\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 142s 2s/step - loss: 0.2210 - accuracy: 0.9398 - val_loss: 1.6952 - val_accuracy: 0.6496\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 145s 2s/step - loss: 0.1483 - accuracy: 0.9464 - val_loss: 2.0021 - val_accuracy: 0.6387\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.1044 - accuracy: 0.9693 - val_loss: 1.7251 - val_accuracy: 0.7007\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0347 - accuracy: 0.9952 - val_loss: 2.0225 - val_accuracy: 0.6861\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0173 - accuracy: 0.9975 - val_loss: 1.8006 - val_accuracy: 0.7117\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0386 - accuracy: 0.9923 - val_loss: 1.8640 - val_accuracy: 0.7226\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0222 - accuracy: 0.9942 - val_loss: 1.8110 - val_accuracy: 0.7226\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 1.7836 - val_accuracy: 0.7117\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 1.9434 - val_accuracy: 0.7226\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 1.9331 - val_accuracy: 0.7044\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 142s 2s/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 1.9440 - val_accuracy: 0.7080\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 142s 2s/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 1.9434 - val_accuracy: 0.7044\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 143s 2s/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 1.9668 - val_accuracy: 0.7044\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 147s 2s/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.9707 - val_accuracy: 0.7117\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 147s 2s/step - loss: 0.0101 - accuracy: 0.9940 - val_loss: 1.9957 - val_accuracy: 0.7117\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 149s 2s/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 2.0018 - val_accuracy: 0.7153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1be5a941e20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.fit(x=train_batches,validation_data=validation_batches,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observstions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since Transfer Learning uses VGG16 which was balically trained on a large amount of data set, we can see a substantial increase in accuracy\n",
    "- VGG16 also provided a better validation accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
